{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:59:59.084180Z",
     "start_time": "2021-05-06T05:59:59.078593Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, avg, lit, udf, count, stddev\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "from data_utils import get_date_range, get_data, clear_data\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:59:59.222235Z",
     "start_time": "2021-05-06T05:59:59.219783Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sqlContext = SQLContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:59:59.555629Z",
     "start_time": "2021-05-06T05:59:59.525995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Category: integer (nullable = true)\n",
      " |-- Timestamp: long (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Query: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentiment 140 Dataset (Tweet + sentiment)\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='false').load('training.1600000.processed.noemoticon.csv', schema='Category INTEGER, Timestamp LONG, Date STRING, Query STRING, Username STRING, Text STRING')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T06:00:08.332639Z",
     "start_time": "2021-05-06T06:00:00.106887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Category| count|\n",
      "+--------+------+\n",
      "|       0|800000|\n",
      "|       4|800000|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show categories (4 = positive, 0 = negative)\n",
    "df = df.where(col(\"Category\").isNotNull())\n",
    "df.groupBy(\"Category\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T06:00:08.342507Z",
     "start_time": "2021-05-06T06:00:08.334642Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(regParam, vocabSize, elasticNetParam, df):\n",
    "    \n",
    "    print(\"regParam: %f, vocabSize: %d, elasticNetParam: %f\" % (regParam, vocabSize, elasticNetParam))\n",
    "\n",
    "    # Tokenizer\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"Text\", outputCol=\"words\", pattern=\"@\\\\S*|\\\\W|http\\\\S*\")\n",
    "\n",
    "    # Stop words\n",
    "    stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "    stopwordsRemover.setStopWords(StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "\n",
    "    # Bag of words\n",
    "    countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=vocabSize, minDF=5)\n",
    "\n",
    "    # String indexer\n",
    "    label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
    "\n",
    "    # Apply the pipeline\n",
    "    pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "    pipelineFit = pipeline.fit(df)\n",
    "    dataset = pipelineFit.transform(df)\n",
    "    \n",
    "    # Training and test splits\n",
    "    (trainingData, testData) = dataset.randomSplit([0.9, 0.1], seed = 100)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    lr = LogisticRegression(maxIter=10, regParam=regParam, elasticNetParam=elasticNetParam)\n",
    "    model = lr.fit(trainingData)\n",
    "    predictions = model.transform(testData)\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "    f1_score = evaluator.evaluate(predictions)\n",
    "    \n",
    "    return [model, pipelineFit, trainingData, testData, f1_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:40:50.674620Z",
     "start_time": "2021-05-06T04:18:58.937760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam: 0.000000, vocabSize: 100, elasticNetParam: 0.000000\n",
      "F1-score: 0.643338\n",
      "regParam: 0.000000, vocabSize: 1000, elasticNetParam: 0.000000\n",
      "F1-score: 0.741295\n",
      "regParam: 0.000000, vocabSize: 10000, elasticNetParam: 0.000000\n",
      "F1-score: 0.776351\n",
      "regParam: 0.000000, vocabSize: 100000, elasticNetParam: 0.000000\n",
      "F1-score: 0.774960\n",
      "regParam: 0.125000, vocabSize: 100, elasticNetParam: 0.000000\n",
      "F1-score: 0.643418\n",
      "regParam: 0.125000, vocabSize: 1000, elasticNetParam: 0.000000\n",
      "F1-score: 0.739739\n",
      "regParam: 0.125000, vocabSize: 10000, elasticNetParam: 0.000000\n",
      "F1-score: 0.773997\n",
      "regParam: 0.125000, vocabSize: 100000, elasticNetParam: 0.000000\n",
      "F1-score: 0.775814\n",
      "regParam: 0.250000, vocabSize: 100, elasticNetParam: 0.000000\n",
      "F1-score: 0.642606\n",
      "regParam: 0.250000, vocabSize: 1000, elasticNetParam: 0.000000\n",
      "F1-score: 0.738675\n",
      "regParam: 0.250000, vocabSize: 10000, elasticNetParam: 0.000000\n",
      "F1-score: 0.772538\n",
      "regParam: 0.250000, vocabSize: 100000, elasticNetParam: 0.000000\n",
      "F1-score: 0.775309\n",
      "regParam: 0.500000, vocabSize: 100, elasticNetParam: 0.000000\n",
      "F1-score: 0.642514\n",
      "regParam: 0.500000, vocabSize: 1000, elasticNetParam: 0.000000\n",
      "F1-score: 0.737575\n",
      "regParam: 0.500000, vocabSize: 10000, elasticNetParam: 0.000000\n",
      "F1-score: 0.770788\n",
      "regParam: 0.500000, vocabSize: 100000, elasticNetParam: 0.000000\n",
      "F1-score: 0.773816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nResults \\n\\nregParam: 0.000000, vocabSize: 100\\nF1 Score: 0.645073\\nregParam: 0.000000, vocabSize: 1000\\nF1 Score: 0.744660\\n> regParam: 0.000000, vocabSize: 10000\\nF1 Score: 0.777977\\nregParam: 0.000000, vocabSize: 100000\\nF1 Score: 0.776525\\nregParam: 0.125000, vocabSize: 100\\nF1 Score: 0.644638\\nregParam: 0.125000, vocabSize: 1000\\nF1 Score: 0.743337\\nregParam: 0.125000, vocabSize: 10000\\nF1 Score: 0.775839\\nregParam: 0.125000, vocabSize: 100000\\nF1 Score: 0.777695\\nregParam: 0.250000, vocabSize: 100\\nF1 Score: 0.643508\\nregParam: 0.250000, vocabSize: 1000\\nF1 Score: 0.742042\\nregParam: 0.250000, vocabSize: 10000\\nF1 Score: 0.774576\\nregParam: 0.250000, vocabSize: 100000\\nF1 Score: 0.777197\\nregParam: 0.500000, vocabSize: 100\\nF1 Score: 0.643352\\nregParam: 0.500000, vocabSize: 1000\\nF1 Score: 0.740815\\nregParam: 0.500000, vocabSize: 10000\\nF1 Score: 0.772755\\nregParam: 0.500000, vocabSize: 100000\\nF1 Score: 0.775840\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find best regParam and vocabSize combination\n",
    "for regParam in [0.0, 0.125, 0.25, 0.5]:\n",
    "    for vocabSize in [100, 1000, 10000, 100000]:\n",
    "        model, pipelineFit, trainingData, testData, f1_score = create_model(regParam, vocabSize, 0.0, df)\n",
    "        print(\"F1-score: %f\" % f1_score)\n",
    "        \n",
    "\"\"\"\n",
    "Results \n",
    "\n",
    "regParam: 0.000000, vocabSize: 100\n",
    "F1 Score: 0.645073\n",
    "regParam: 0.000000, vocabSize: 1000\n",
    "F1 Score: 0.744660\n",
    "> regParam: 0.000000, vocabSize: 10000\n",
    "F1 Score: 0.777977\n",
    "regParam: 0.000000, vocabSize: 100000\n",
    "F1 Score: 0.776525\n",
    "regParam: 0.125000, vocabSize: 100\n",
    "F1 Score: 0.644638\n",
    "regParam: 0.125000, vocabSize: 1000\n",
    "F1 Score: 0.743337\n",
    "regParam: 0.125000, vocabSize: 10000\n",
    "F1 Score: 0.775839\n",
    "regParam: 0.125000, vocabSize: 100000\n",
    "F1 Score: 0.777695\n",
    "regParam: 0.250000, vocabSize: 100\n",
    "F1 Score: 0.643508\n",
    "regParam: 0.250000, vocabSize: 1000\n",
    "F1 Score: 0.742042\n",
    "regParam: 0.250000, vocabSize: 10000\n",
    "F1 Score: 0.774576\n",
    "regParam: 0.250000, vocabSize: 100000\n",
    "F1 Score: 0.777197\n",
    "regParam: 0.500000, vocabSize: 100\n",
    "F1 Score: 0.643352\n",
    "regParam: 0.500000, vocabSize: 1000\n",
    "F1 Score: 0.740815\n",
    "regParam: 0.500000, vocabSize: 10000\n",
    "F1 Score: 0.772755\n",
    "regParam: 0.500000, vocabSize: 100000\n",
    "F1 Score: 0.775840\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:42:09.204342Z",
     "start_time": "2021-05-06T04:40:50.677898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam: 0.000000, vocabSize: 10000, elasticNetParam: 0.000000\n"
     ]
    }
   ],
   "source": [
    "model, pipelineFit, trainingData, testData, f1_score = create_model(0.0, 10000, 0.0, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:13:19.173517Z",
     "start_time": "2021-05-06T05:13:14.056610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|         probability|prediction|                Text|\n",
      "+--------------------+----------+--------------------+\n",
      "|[0.06269704531033...|       1.0|Your code sucks a...|\n",
      "|[0.94724971501712...|       0.0|I love your libra...|\n",
      "|[0.11565798243482...|       1.0|Why did you choos...|\n",
      "|[0.92590959580645...|       0.0|Yes, thank you ve...|\n",
      "|[0.48689805666691...|       1.0|I should be aroun...|\n",
      "+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"name\", \"Your code sucks and you will never get a job.\"),\n",
    "        (\"name\", \"I love your library, very excellent code sir.\"),\n",
    "        (\"name\", \"Why did you choose Java? It is so f***ing slow and terrible.\"),\n",
    "        (\"name\", \"Yes, thank you very much friend.\"),\n",
    "        (\"name\", \"I should be around 0.5.\"),\n",
    "    ],\n",
    "    [\"Name\", \"Text\"]\n",
    ")\n",
    "\n",
    "model.transform(pipelineFit.transform(test_df)).select(col('probability'), col('prediction'), col('Text')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:30:53.035092Z",
     "start_time": "2021-05-07T11:30:53.013739Z"
    }
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='false').load('data_april2021.csv', schema='Repo STRING, Language STRING, Comment STRING, Forks_Count INTEGER, Stargazers_count INTEGER, Open_issues_count INTEGER, Date TIMESTAMP')\n",
    "\n",
    "df.createOrReplaceTempView(\"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:30:53.728356Z",
     "start_time": "2021-05-07T11:30:53.714773Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_js_data():    \n",
    "    lang_df = spark.sql(\"SELECT Repo, Comment as Text, Language FROM events WHERE Language IN ('JavaScript')\").dropna()\n",
    "\n",
    "    # Sentiment\n",
    "    union_df = model.transform(pipelineFit.transform(lang_df)).select(col('probability'), col('Language'), col('Text'))\n",
    "    # Pandas\n",
    "    pd = union_df.toPandas()\n",
    "    s = pd['probability'].apply(lambda x: pandas.Series(x.toArray()))\n",
    "    pd['p'] = 1 - s[0]\n",
    "    pd = pd.drop('probability', 1)\n",
    "    \n",
    "    return pd\n",
    "\n",
    "def get_java_data():    \n",
    "    lang_df = spark.sql(\"SELECT Repo, Comment as Text, Language FROM events WHERE Language IN ('Java')\").dropna()\n",
    "\n",
    "    # Sentiment\n",
    "    union_df = model.transform(pipelineFit.transform(lang_df)).select(col('probability'), col('Language'), col('Text'))\n",
    "    # Pandas\n",
    "    pd = union_df.toPandas()\n",
    "    s = pd['probability'].apply(lambda x: pandas.Series(x.toArray()))\n",
    "    pd['p'] = 1 - s[0]\n",
    "    pd = pd.drop('probability', 1)\n",
    "    \n",
    "    return pd\n",
    "\n",
    "def get_python_data():    \n",
    "    lang_df = spark.sql(\"SELECT Repo, Comment as Text, Language FROM events WHERE Language IN ('Python')\").dropna()\n",
    "\n",
    "    # Sentiment\n",
    "    union_df = model.transform(pipelineFit.transform(lang_df)).select(col('probability'), col('Language'), col('Text'))\n",
    "    # Pandas\n",
    "    pd = union_df.toPandas()\n",
    "    s = pd['probability'].apply(lambda x: pandas.Series(x.toArray()))\n",
    "    pd['p'] = 1 - s[0]\n",
    "    pd = pd.drop('probability', 1)\n",
    "    \n",
    "    return pd\n",
    "\n",
    "\n",
    "def get_google_data():    \n",
    "    google_df = spark.sql(\"SELECT Repo, Comment as Text FROM events WHERE Repo LIKE '%google/%'\").dropna()\n",
    "    \n",
    "    # Sentiment\n",
    "    union_df =  model.transform(pipelineFit.transform(google_df)).select(col('probability'), col('Repo'), col('Text'))\n",
    "    # Pandas\n",
    "    pd = union_df.toPandas()\n",
    "    s = pd['probability'].apply(lambda x: pandas.Series(x.toArray()))\n",
    "    pd['p'] = 1 - s[0]\n",
    "    pd = pd.drop('probability', 1)\n",
    "    \n",
    "    return pd\n",
    "\n",
    "def get_amazon_data():    \n",
    "    amazon_df = spark.sql(\"SELECT Repo, Comment as Text FROM events WHERE Repo LIKE '%aws/%'\").dropna()\n",
    "    \n",
    "    # Sentiment\n",
    "    union_df =  model.transform(pipelineFit.transform(amazon_df)).select(col('probability'), col('Repo'), col('Text'))\n",
    "    # Pandas\n",
    "    pd = union_df.toPandas()\n",
    "    s = pd['probability'].apply(lambda x: pandas.Series(x.toArray()))\n",
    "    pd['p'] = 1 - s[0]\n",
    "    pd = pd.drop('probability', 1)\n",
    "    return pd\n",
    "\n",
    "def get_facebook_data():    \n",
    "    facebook_df = spark.sql(\"SELECT Repo, Comment as Text FROM events WHERE Repo LIKE '%facebook/%'\").dropna()\n",
    "    \n",
    "    union_df = model.transform(pipelineFit.transform(facebook_df)).select(col('probability'), col('Repo'), col('Text'))\n",
    "    # Pandas\n",
    "    pd = union_df.toPandas()\n",
    "    s = pd['probability'].apply(lambda x: pandas.Series(x.toArray()))\n",
    "    pd['p'] = 1 - s[0]\n",
    "    pd = pd.drop('probability', 1)\n",
    "    \n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:30:54.412847Z",
     "start_time": "2021-05-07T11:30:54.410828Z"
    }
   },
   "outputs": [],
   "source": [
    "# results = [['Python', 0.510208], ['Go', 0.574086], ['JavaScript',0.521960], ['C++',0.404080], ['Java', 0.296523]]\n",
    "# df = pandas.DataFrame(results, columns = ['Name', 'P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:31:04.541479Z",
     "start_time": "2021-05-07T11:30:55.003910Z"
    }
   },
   "outputs": [],
   "source": [
    "lang = get_facebook_data()\n",
    "lang.to_csv('facebook2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:31:09.135793Z",
     "start_time": "2021-05-07T11:31:04.543491Z"
    }
   },
   "outputs": [],
   "source": [
    "lang = get_amazon_data()\n",
    "lang.to_csv('amazon2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:31:13.522884Z",
     "start_time": "2021-05-07T11:31:09.138098Z"
    }
   },
   "outputs": [],
   "source": [
    "lang = get_google_data()\n",
    "lang.to_csv('google2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:31:41.675346Z",
     "start_time": "2021-05-07T11:31:13.524840Z"
    }
   },
   "outputs": [],
   "source": [
    "lang = get_js_data()\n",
    "lang.to_csv('js2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:32:14.388580Z",
     "start_time": "2021-05-07T11:31:41.677852Z"
    }
   },
   "outputs": [],
   "source": [
    "lang = get_python_data()\n",
    "lang.to_csv('python2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:32:37.253632Z",
     "start_time": "2021-05-07T11:32:14.391053Z"
    }
   },
   "outputs": [],
   "source": [
    "lang = get_java_data()\n",
    "lang.to_csv('java2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
